# ğŸ•¸ï¸ Rust Web Scraper â€” Extract All Links from a Webpage

A simple asynchronous Rust program that fetches a webpage and prints all hyperlinks (`<a href="...">`) found within it.  
Built using **reqwest**, **select**, and **error_chain**, this project demonstrates web scraping fundamentals in Rust with clean error handling and async execution.

---

## ğŸš€ Features
- ğŸŒ Fetches HTML content from any given URL
- ğŸ§  Parses HTML using the [`select`](https://crates.io/crates/select) crate
- ğŸ”— Extracts all `<a href="...">` links
- âš™ï¸ Uses `error_chain` for easy error management
- âš¡ Runs asynchronously with `tokio`

---

## ğŸ§© Dependencies
This project uses:
- [`reqwest`](https://crates.io/crates/reqwest) â€” async HTTP client
- [`select`](https://crates.io/crates/select) â€” HTML document parser
- [`error-chain`](https://crates.io/crates/error-chain) â€” simplified error handling
- [`tokio`](https://crates.io/crates/tokio) â€” async runtime
